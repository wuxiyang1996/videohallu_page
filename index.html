<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>VideoHallu: Evaluating and Mitigating Multi-modal Hallucinations for Synthetic Videos</title>


  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">VideoHallu: Evaluating and Mitigating Multi-modal Hallucinations for Synthetic Videos</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://zli12321.github.io">Zongxia Li</a><sup>1*</sup>,</span>
            <span class="author-block">
              <a href="https://wuxiyang1996.github.io/">Xiyang Wu</a><sup>1*</sup>,</span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/yubin-qin/">Yubin Qin</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://github.com/SmashedPython">Hongyang Du</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://guangyaoshi.github.io/">Guangyao Shi</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://github.com/SmashedPython">Hongyang Du</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.cs.umd.edu/people/dmanocha">Tianyi Zhou</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://users.umiacs.umd.edu/~ying/">Jordan Lee Boyd-Graber</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Maryland, College Park, <sup>2</sup>University of Southern California</span><br>
            
            <span class="author-block"><sup>*</sup>Equal Contribution</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://github.com/zli12321/VideoHallu/blob/main/paper.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/zli12321/VideoHallu/blob/main/paper.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/zli12321/VideoHallu"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
                <span class="link-block">
              <a href="https://huggingface.co/datasets/zli12321/VideoHalluB" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <img src="images/hf-logo.svg" style="display:block;width:330px;height:240px" />
                </span>
                <span>Dataset</span>
              </a>
            </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <!-- First Image with adjusted size and centered -->
      <div style="margin-bottom: 0px; text-align: center;"> <!-- Adjust the margin as needed -->
        <img src="images/teaser.png" alt="Teaser Image" style="width: 1000px; display: block; margin: auto;"> <!-- Adjust width as needed -->

        <!-- Caption with constrained width -->
        <div style="max-width: 800px; margin: auto;"> <!-- Adjust max-width as needed -->
          <h2 class="subtitle has-text-justified">
            <br>
            <strong>VideoHallu</strong> is a synthetic video benchmark with QA pairs requiring human-level reasoning. Evaluating and post-training SoTA MLLMs on commonsense/physics data shows its impact on improving model reasoning.
          </h2>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Abstract -->
<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
          Synthetic video generation using foundation models has gained significant attention due to its realism and broad applications. However, while these models excel at generating visually coherent and high-quality video frames, they often overlook commonsense reasoning and physical law violations, leading to abnormal content. Existing score-based evaluations like VideoScore mainly focus on general video quality and do not take these abnormalities into account, and offer no explanations of the evaluation results. A more promising evaluation approach is to leverage multi-modal large language models (MLLMs) as interpretable video evaluators, following the approach of FActScore. <strong>However, how well MLLMs can detect these abnormalities in synthetic videos is underexplored.</strong>
          </p>
          <p>
          Motivated by a more interpretable video generation evaluation, we introduce <strong>VideoHallu</strong>, a benchmark built from synthetic videos produced by popular models like Sora, Veo2, Kling, paired with expert-crafted question-answering pair examples easily solvable with human-level perception and reasoning across multiple categories. We evaluate several State-of-the-Art (SoTA) MLLMs with our benchmark, including GPT-4o, Gemini-2.5-Pro, Qwen-2.5-VL, and forefront models like Video-R1 and VideoChat-R1. Despite the strong performance of R1 MLLMs on real-world video benchmarks like MVBench and MovieChat, these models still struggle and hallucinate on basic commonsense and physics reasoning tasks in synthetic videos, highlighting <strong>synthetic video hallucination</strong> as an underexplored challenge.
          </p>
          <p>
          Moreover, we post-train current SoTA MLLMs, Qwen-2.5-VL-7B, with Group Relative Policy Optimization (GRPO) using both real-world and synthetic commonsense/physics datasets. Our results show improved overall accuracy compared to the base model, achieving the highest performance among all models, highlighting the importance of <strong>integrating high-quality counterexamples</strong> to enhance <strong>commonsense and physics reasoning</strong> in MLLMs' language priors.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section hero is-small">
  <div class="hero-body">
    <div class="container">
    <h2 class="title is-3">Benchmark</h2>
      <p>We design our benchmark, VideoHallu, around four question categories aimed at probing hallucinations in synthetic video understanding, organized by the level of reasoning required from MLLMs to perform video-question answering in practice. The benchmark spans from perceptual understanding to high-level abstract reasoning.
      </p>
      <br>
      <p>
          <strong>(a) Alignment</strong> checks if the model correctly identifies and understands entities using visual and textual cues.
      </p>

      <p>
          <strong>(b) Spatial-temporal Consistency</strong> examines whether the model can track entity motion across frames.
      </p>
      <p>
          <strong>(c) Common Sense Reasoning</strong> tests if the model can reason based on its knowledge.
      </p>
      <p>
          <strong>(d) Physics</strong> assesses if the model applies physical laws to entity motions and procedural understanding.
      </p>
      <br>
      <div class="image">
        <img src="images/fig1.png" alt="Teaser Image" style="width: 1000px; display: block; margin: auto;">
        </div>
            </div>
    </div>
</section>

<section class="section hero is-small">
  <div class="container">
    <h2 class="title is-3">The Dawn of MLLMs in Synthetic Videos</h2>

    <p>We present selected cases from SoTA MLLM evaluations across each category. Hallucinations in model answers, common sense or physics violations in videos, and other notable cues in the video, questions, or ground truth are highlighted to assist the reader's understanding. More examples can be found in the Appendix of our paper.</p>
    <br>
    <p><strong>Note:</strong> The legend below explains all the symbols used to represent the State-of-the-Art (SoTA) MLLMs featured in our showcases for synthetic video generation and video question-answering.</p>
    <div style="text-align: center;">
    <img src="images/legend.png" width="700" alt="legend"></div>
    <br>
    <h4 class="title is-4">Alignment</h4>
    <p><strong>Video Generation Prompt:</strong>  A young male athlete is playing basketball on an outdoor court, performing impressive dribbling and slam dunks.
    </p><br>

    <p><strong>Synthetic Video:</strong></p>
    <div style="text-align: center;">
      <img src="images/alignment.gif" width="700" alt="Alignment"><br><br>
    </div>
    <p><strong>Video Question-Answering by MLLMs:</strong></p>
    <div style="text-align: center;">
      <img src="./images/alignment.png" width="700" alt="alignment_vqa">
    </div>
<br>
    <h4 class="title is-4">Spatial-temporal Consistency</h4>
    <p><strong>Video Generation Prompt:</strong> Generate a quail and a rooster celebrating New Year.
    </p><br>

    <p><strong>Synthetic Video:</strong></p>
    <div style="text-align: center;">
      <img src="images/rooster.gif" width="700" alt="STC"><br><br>
    </div>
    <p><strong>Video Question-Answering by MLLMs:</strong></p>
    <div style="text-align: center;">
      <img src="./images/STC.png" width="700" alt="stc_vqa">
    </div><br>

    <h4 class="title is-4">Common Sense Reasoning</h4>
    <p><strong>Video Generation Prompt:</strong> A feather and a heavy rock are released at the same height and begin to fall to the ground on Earth.
    </p><br>
    <p><strong>Synthetic Video:</strong></p>
    <div style="text-align: center;">
      <img src="images/feather_veo2.gif" width="700" alt="CSR"><br><br>
    </div>
    <p><strong>Video Question-Answering by MLLMs:</strong></p>
    <div style="text-align: center;">
      <img src="./images/CSR.png" width="700" alt="stc_vqa">
    </div>
    <br>

    <h4 class="title is-4">Physics</h4>
    <p><strong>Video Generation Prompt:</strong> Generate the sequence showing a bullet being shot into a watermelon.
    </p><br>
    <p><strong>Synthetic Video:</strong></p>
    <div style="text-align: center;">
      <img src="images/watermelon_explode-ezgif.com-video-to-gif-converter.gif" width="700" alt="CSR"><br><br>
    </div>
    <p><strong>Video Question-Answering by MLLMs:</strong></p>
    <div style="text-align: center;">
      <img src="./images/P.png" width="700" alt="stc_vqa">
    </div>

  </div>
</section>

<section class="section hero is-small">
  <div class="hero-body">
    <div class="container">
    <h2 class="title is-3">Evaluation over SoTA MLLMs</h2>
      <p>We evaluate diverse SoTA models across sizes and training strategies, reporting both overall and sub-category accuracies. Qwen2.5-VL-32B achieves the highest overall performance among all models.
      </p>
      <div style="text-align: center;">
        <img src="./images/all_results.png" width="1000" alt="stc_vqa">
      </div>
      <br>
      <p>
         We evaluate SoTA MLLMs on VideoHallu, with results broken down by sub-category. From left to right, we show: (a) models under 7B parameters; (b) models between 7B–38B; (c) R1 fine-tuned models; and (d) large black-box MLLMs. While many perform well on alignment tasks, they remain prone to hallucinations in reasoning-heavy tasks, with notably weaker performance on physics and commonsense reasoning.
      </p>
      <div style="text-align: center;">
        <img src="./images/all_radar.png" width="1200" alt="stc_vqa">
      </div>
      </div>
    </div>
</section>

<section class="section hero is-small">
  <div class="hero-body">
    <div class="container">
    <h2 class="title is-3">Fine-tuning Results</h2>
      <p>We evaluate models fine-tuned on either domain-specific sub-datasets or curriculum-based composite datasets. Results show that models trained only on general real-world videos yield little to no gains on synthetic video understanding. Incorporating general physics data improves physics reasoning, and a curriculum starting with real-world physics followed by synthetic data leads to a 2.8% performance boost.
      </p>
      <div style="text-align: center;">
        <img src="./images/ft_results.png" width="1000" alt="stc_vqa">
      </div>
      <br>
      <p>
         We show results for (a) previous SoTA MLLMs, (b) models fine-tuned on sub-datasets, and (c) models fine-tuned on the full dataset via curriculum learning. Compared to the baseline (Qwen2.5-VL-7B), reinforcement fine-tuning on commonsense and physics data improves models' reasoning and overall performance in synthetic video understanding.
      </p>
      <div style="text-align: center;">
        <img src="./images/ft_radar.png" width="900" alt="stc_vqa">
      </div>
      </div>
    </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
